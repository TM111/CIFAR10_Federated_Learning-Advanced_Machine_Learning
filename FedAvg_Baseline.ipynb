{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DWwypvPVTp5",
        "outputId": "0d01faa9-3000-400a-aa8d-9b5a08a37b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.10.0+cu111)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision) (3.10.0.2)\n",
            "Requirement already satisfied: Pillow-SIMD in /usr/local/lib/python3.7/dist-packages (9.0.0.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install 'torch'\n",
        "!pip3 install 'torchvision'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW98mYNis-RX"
      },
      "source": [
        "PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5mYvkqes9ob"
      },
      "outputs": [],
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "\n",
        "NUM_CLASSES = 10   \n",
        "\n",
        "BATCH_SIZE = 64     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "SERVER_LR=1   # server learning rate\n",
        "\n",
        "LR = 0.01       # clients Learning Rate\n",
        "\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 4e-4  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 1   # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 5    # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 30\n",
        "\n",
        "MODEL = \"LeNet5\"   #change model (LeNet5, mobilenetV2)\n",
        "\n",
        "FL_MODE=1      #set this variable to skip the centralized model\n",
        "\n",
        "DISTRIBUTION=3 #type of distribution of the dataset -> 1=dirichlet distribution    2=iid distribution        3=non_iid ditribution\n",
        "\n",
        "ALPHA='1.00' #alpha value for dirichlet distribution -> '0.00', '0.05', '0.10', '0.20', '0.50', '1.00', '10.00', '100.00'\n",
        "\n",
        "NUM_CLASS_RANGE=[1,7]  #range class number for non_iid (distribution 3). (1,4) is default. We can test also (1,7) or (1,10)\n",
        "\n",
        "NUM_CLIENTS = 100\n",
        "\n",
        "NUM_SELECTED_CLIENTS = 3 #NUM CLIENTS FOR ROUND\n",
        "\n",
        "ROUNDS = 20\n",
        "\n",
        "BATCH_NORM = 1 # use batch normalization layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnq8b0oIWRDB"
      },
      "source": [
        "PREPARE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy3LuyaqWQft",
        "outputId": "e745bd95-38dd-4977-ac1b-b473534d1500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Train Dataset: 50000\n",
            "Test Dataset: 10000\n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms as transforms\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "\n",
        "train_transform =  transforms.Compose([transforms.ToTensor()])\n",
        "test_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='./CIFAR10', train=True, download=True, transform=train_transform)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./CIFAR10', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_set)))\n",
        "print('Test Dataset: {}'.format(len(test_set)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZmtPZcHH8fg"
      },
      "source": [
        "**DEFINE MODELS**\n",
        "\n",
        "*Here we define the networks (LeNet5, mobilenetV2)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sqkE-UQDapv"
      },
      "outputs": [],
      "source": [
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "from statistics import mean \n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as func\n",
        "\n",
        "#DEFINE NETWORKS\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = func.relu(self.conv1(x))\n",
        "        x = func.max_pool2d(x, 2)\n",
        "        x = func.relu(self.conv2(x))\n",
        "        x = func.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = func.relu(self.fc1(x))\n",
        "        x = func.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class LeNet5_batchNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5_batchNorm, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
        "        self.conv1_bn=nn.BatchNorm2d(6)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.conv2_bn=nn.BatchNorm2d(16)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = func.relu(self.conv1_bn(x))\n",
        "        x = func.max_pool2d(x, 2)\n",
        "        x = self.conv2(x)\n",
        "        x = func.relu(self.conv2_bn(x))\n",
        "        x = func.max_pool2d(x, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = func.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = func.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def mobilenetV2(pretrain=True):\n",
        "  model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=pretrain)\n",
        "  return model\n",
        "\n",
        "def get_net():\n",
        "  if(MODEL==\"LeNet5\"):\n",
        "    if(BATCH_NORM == 1):\n",
        "      return LeNet5_batchNorm()\n",
        "    else:\n",
        "      return LeNet5()\n",
        "\n",
        "  elif (MODEL==\"mobilenetV2\"):\n",
        "    if(BATCH_NORM == 1):\n",
        "      return mobilenetV2()\n",
        "    else:\n",
        "      return mobilenetV2()\n",
        "\n",
        "#DEFINE TEST FUNCTION\n",
        "def evaluate(net, criterion,dataloader,  print_tqdm = True):\n",
        "  with torch.no_grad():\n",
        "    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "    net.train(False) # Set Network to evaluation mode\n",
        "    running_corrects = 0\n",
        "    #iterable = tqdm(dataloader) if print_tqdm else dataloader\n",
        "    losses = []\n",
        "    for images, labels in dataloader: \n",
        "      images = images.to(DEVICE)\n",
        "      labels = labels.to(DEVICE)\n",
        "      # Forward Pass\n",
        "      outputs = net(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      losses.append(loss.item())\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "      # Update Corrects\n",
        "      running_corrects = running_corrects + torch.sum(preds == labels.data).data.item()\n",
        "    # Calculate Accuracy\n",
        "    accuracy = running_corrects / float(len(dataloader.dataset))\n",
        "  return mean(losses),accuracy\n",
        "\n",
        "centralized_accuracy=\"none\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytsl8NJ8Y3Z3"
      },
      "source": [
        "**CENTRALIZED MODEL**\n",
        "\n",
        "experiment with a standard approach. Create a model and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgMRrHW9iKr_"
      },
      "outputs": [],
      "source": [
        "#STANDARD APPROACH\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.backends import cudnn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from statistics import mean \n",
        "import torch.nn.functional as func\n",
        "\n",
        "if(FL_MODE==0):\n",
        "  centralized_model = get_net()\n",
        "\n",
        "  centralized_optimizer = torch.optim.SGD(centralized_model.parameters(), lr=LR, momentum=MOMENTUM)\n",
        "  centralized_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  centralized_model = centralized_model.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "  cudnn.benchmark # Calling this optimizes runtime\n",
        "\n",
        "\n",
        "# Start iterating over the epochs\n",
        "for epoch in range(0,NUM_EPOCHS):\n",
        "  if(FL_MODE==1):\n",
        "    break\n",
        "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS))\n",
        "  # Iterate over the dataset\n",
        "  for images, labels in train_loader:\n",
        "    # Bring data over the device of choice\n",
        "    images = images.to(DEVICE)\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    centralized_model.train() # Sets module in training mode\n",
        "\n",
        "    centralized_optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "    # Forward pass to the network\n",
        "    outputs = centralized_model(images)\n",
        "    # Compute loss based on output and ground truth\n",
        "    loss = centralized_criterion(outputs, labels)\n",
        "\n",
        "    # Compute gradients for each layer and update weights\n",
        "    loss.backward()  # backward pass: computes gradients\n",
        "    centralized_optimizer.step() # update weights based on accumulated gradients\n",
        "  loss,accuracy = evaluate(centralized_model,centralized_criterion, test_loader)\n",
        "  print('\\nTest Accuracy: {}'.format(accuracy)) \n",
        "\n",
        "  centralized_accuracy=accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJbAALaLP6eA"
      },
      "source": [
        "DIRICHLET DISTRIBUTION\n",
        "\n",
        "Here we have the split of dataset based on dirichlet distribution\n",
        "\n",
        "https://github.com/google-research/google-research/tree/master/federated_vision_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ9I5VDpP1ML"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import csv\n",
        "from os import path\n",
        "import os\n",
        "import urllib.request \n",
        "import zipfile\n",
        "import random\n",
        "\n",
        "\n",
        "def cifar_parser(line, is_train=True):\n",
        "  if is_train:\n",
        "    user_id, image_id, class_id = line\n",
        "    return user_id, image_id, class_id\n",
        "  else:\n",
        "    image_id, class_id = line\n",
        "    return image_id, class_id\n",
        "\n",
        "\n",
        "def dirichlet_distribution(alpha):    # generate trainset split from csv\n",
        "  #download csv files\n",
        "  url=\"http://storage.googleapis.com/gresearch/federated-vision-datasets/cifar10_v1.1.zip\"\n",
        "  dir=\"/content/cifar10_csv\"\n",
        "  try:\n",
        "    os.mkdir(dir)\n",
        "  except:\n",
        "    print(\"Folder already exist\")\n",
        "\n",
        "  urllib.request.urlretrieve(url, \"/content/cifar10_csv/cifar.zip\")\n",
        "  with zipfile.ZipFile(\"/content/cifar10_csv/cifar.zip\",\"r\") as zip_ref:\n",
        "      zip_ref.extractall(\"/content/cifar10_csv\")\n",
        "\n",
        "  train_file=\"cifar10_csv/federated_train_alpha_\"+alpha+\".csv\"\n",
        "  \"\"\"Inspects the federated train split.\"\"\"\n",
        "  print('Train file: %s' % train_file)\n",
        "  if not path.exists(train_file):\n",
        "    print('Error: file does not exist.')\n",
        "    return\n",
        "  user_images={}\n",
        "  with open(train_file) as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)  # skip header.\n",
        "    for line in reader:\n",
        "      user_id, image_id, class_id = cifar_parser(line, is_train=True)\n",
        "      if(user_id not in user_images.keys()):\n",
        "        user_images[user_id]=[]\n",
        "      user_images[user_id].append(int(image_id))\n",
        "  return user_images\n",
        "\n",
        "def cifar_iid(): # all clients have all classes with the same data distribution\n",
        "  user_images={}\n",
        "  classes_dict={}\n",
        "  for i in range(len(train_set)):\n",
        "    label=train_set[i][1]\n",
        "    if(label not in classes_dict.keys()):\n",
        "      classes_dict[label]=[]\n",
        "    classes_dict[label].append(i)\n",
        "  classes_index=[]\n",
        "  for label in classes_dict.keys():\n",
        "    classes_index=classes_index+classes_dict[label]\n",
        "\n",
        "  count=0\n",
        "  for i in classes_index:\n",
        "    if(str(count) not in user_images.keys()):\n",
        "      user_images[str(count)]=[]\n",
        "    user_images[str(count)].append(i)\n",
        "    count=count+1\n",
        "    if(count==NUM_CLIENTS):\n",
        "      count=0\n",
        "  return user_images\n",
        "\n",
        "def cifar_noniid(): # all clients have a number of class beetwen 1 and 4 with the same data distribution\n",
        "  user_images=cifar_iid()\n",
        "  for key in user_images.keys():\n",
        "    n_classes=random.randint(NUM_CLASS_RANGE[0],NUM_CLASS_RANGE[1])\n",
        "    list_of_class=random.sample(range(0, NUM_CLASSES), n_classes)\n",
        "    new_index_list=[]\n",
        "    for i in user_images[key]:\n",
        "      label=int(train_set[i][1])\n",
        "      if(label in list_of_class):\n",
        "        new_index_list.append(i)\n",
        "    user_images[key]=new_index_list\n",
        "  return user_images\n",
        "\n",
        "\n",
        "distribution=DISTRIBUTION\n",
        "alpha=ALPHA\n",
        "\n",
        "if(distribution==1):  # https://github.com/google-research/google-research/tree/master/federated_vision_datasets\n",
        "  train_user_images=dirichlet_distribution(alpha)\n",
        "\n",
        "elif(distribution==2):\n",
        "  train_user_images=cifar_iid()\n",
        "\n",
        "elif(distribution==3):  # https://towardsdatascience.com/preserving-data-privacy-in-deep-learning-part-2-6c2e9494398b\n",
        "  train_user_images=cifar_noniid()\n",
        "\n",
        "\n",
        "\n",
        "train_loader_list={}   #TRAIN LOADER DICT\n",
        "for user_id in train_user_images.keys():\n",
        "  dataset_ = torch.utils.data.Subset(train_set, train_user_images[user_id])\n",
        "  dataloader = torch.utils.data.DataLoader(dataset=dataset_, batch_size=BATCH_SIZE, shuffle=False)\n",
        "  train_loader_list[user_id]=dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olLeAhvWrDjH"
      },
      "source": [
        "DEFINE CLIENTS\n",
        "\n",
        "Creation of pool of clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HnJJrP3rHZd",
        "outputId": "c2884bfb-77f6-468a-9b6f-21c70427ebe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution of trainset for client 2 [0, 0, 0, 0, 0, 50, 50, 0, 0, 0]\n",
            "Distribution of testset for client  2 [0, 0, 0, 0, 0, 8, 8, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "def generated_test_distribution(classes, num_samples):    # generate testset with specific class and size\n",
        "  test_user_images=[]\n",
        "  count=0\n",
        "  for c in classes:\n",
        "    count=count+1\n",
        "    indexes=list(range(len(test_set)))\n",
        "    for i in range(random.randint(2,7)):\n",
        "      random.shuffle(indexes)\n",
        "    for i in indexes:\n",
        "      if(test_set[i][1]==c):\n",
        "        test_user_images.append(i)\n",
        "      if(len(test_user_images)==int(num_samples/len(classes)+1)*count):\n",
        "        break\n",
        "  dataset_ = torch.utils.data.Subset(test_set, test_user_images)\n",
        "  dataloader = torch.utils.data.DataLoader(dataset=dataset_, batch_size=BATCH_SIZE, shuffle=False)\n",
        "  return dataloader\n",
        "\n",
        "\n",
        "#Client datastructure\n",
        "class Client():\n",
        "    def __init__(self, id,net,train_lr,optimizer,criterion):\n",
        "        self.id = id\n",
        "        self.net=net\n",
        "        self.train_loader=train_lr\n",
        "\n",
        "        classes=[]\n",
        "        for data in train_lr.dataset:\n",
        "          if(data[1] not in classes):\n",
        "            classes.append(data[1])\n",
        "        self.test_loader=generated_test_distribution(classes, int(len(train_lr.dataset)*0.15)) #specific testset for each clients\n",
        "\n",
        "        self.optimizer=optimizer\n",
        "        self.criterion=criterion\n",
        "        if(len(train_lr.dataset)%100==0):    # an epoch for every 100 images\n",
        "          self.local_epoch=int(len(train_lr.dataset)/100)\n",
        "        else:\n",
        "          self.local_epoch=int(len(train_lr.dataset)/100)+1\n",
        "        \n",
        "        self.updates=0    #Δθ memorizes the updates after training\n",
        "\n",
        "\n",
        "#Istance list of clients\n",
        "clients_list=[]\n",
        "for i in range(NUM_CLIENTS):\n",
        "  if(FL_MODE==0):\n",
        "    break\n",
        "  net=get_net()\n",
        "  opt = torch.optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM)\n",
        "  crt=nn.CrossEntropyLoss()\n",
        "  client=Client(i,net,train_loader_list[str(i)],opt,crt)\n",
        "  clients_list.append(client)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ind=2\n",
        "l=[0,0,0,0,0,0,0,0,0,0]\n",
        "for i in range(len(clients_list[ind].train_loader.dataset)):\n",
        "  if(FL_MODE==0):\n",
        "    break\n",
        "  index=clients_list[ind].train_loader.dataset[i][1]\n",
        "  l[index]=l[index]+1\n",
        "print(\"Distribution of trainset for client \"+str(ind),l)\n",
        "\n",
        "l=[0,0,0,0,0,0,0,0,0,0]\n",
        "for i in range(len(clients_list[ind].test_loader.dataset)):\n",
        "  if(FL_MODE==0):\n",
        "    break\n",
        "  index=clients_list[ind].test_loader.dataset[i][1]\n",
        "  l[index]=l[index]+1\n",
        "print(\"Distribution of testset for client  \"+str(ind),l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvQ3af1Cw887"
      },
      "source": [
        "FEDERATED AVG FUNCTIONS\n",
        "\n",
        "The functions to send and calculate the avg models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4qFgEvnsFFm"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import random\n",
        "#https://github.com/AshwinRJ/Federated-Learning-PyTorch/blob/master/src/utils.py\n",
        "\n",
        "#Calculate AVG for each clients layers\n",
        "def average_weights(updates,clients):           # AggregateClient()\n",
        "    #IMPLEMENTATION OF FEDAVG\n",
        "    total_samples=0\n",
        "    for c in clients:\n",
        "      total_samples=total_samples+len(c.train_loader.dataset)\n",
        "\n",
        "    updates_avg = copy.deepcopy(updates[0])\n",
        "    for key in updates_avg.keys():\n",
        "        updates_avg[key] = updates_avg[key]*0\n",
        "\n",
        "    for key in updates_avg.keys():\n",
        "        for i in range(0, len(updates)):\n",
        "            updates_avg[key] = updates_avg[key] + updates[i][key]*len(clients[i].train_loader.dataset)\n",
        "        updates_avg[key] = torch.div(updates_avg[key], total_samples)\n",
        "    return updates_avg\n",
        "\n",
        "\n",
        "#CLIENTS -> MAIN MODEL & AVERAGE\n",
        "def set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,clients):\n",
        "  local_updates=[]\n",
        "  for c in clients:\n",
        "    local_updates.append(copy.deepcopy(c.updates))\n",
        "\n",
        "  updates = average_weights(local_updates,clients) # AggregateClient()\n",
        "\n",
        "  w=main_model.state_dict()\n",
        "  for key in w.keys():\n",
        "    updates[key] = w[key]+SERVER_LR*updates[key]    # θt+1 ← θt - γgt\n",
        "  main_model.load_state_dict(copy.deepcopy(updates))\n",
        "  return main_model\n",
        "\n",
        "#PRINTS FOR DEBUG\n",
        "def printWeights(clients,main_model):   # test to view if the algotihm is correct\n",
        "    w=[]\n",
        "    for c in clients:\n",
        "      w.append(c.net.state_dict())\n",
        "    w_avg=main_model.state_dict()\n",
        "    if(MODEL=='LeNet5'):\n",
        "      s=''\n",
        "      for i in range(len(clients)):\n",
        "        s=s+'size '+str(len(clients[i].train_loader.dataset))+' '+str(w[i][\"conv1.weight\"][0][0][0][0])+'     '\n",
        "      print(s)\n",
        "      print('avg '+str(w_avg[\"conv1.weight\"][0][0][0][0]))\n",
        "    elif(MODEL=='mobilenetV2'):\n",
        "      s=''\n",
        "      for i in range(len(clients)):\n",
        "        s=s+'size '+str(len(clients[i].train_loader.dataset))+' '+str(w[i][\"features.2.conv.1.1.weight\"][0])+'     '\n",
        "      print(s)\n",
        "      print('avg '+str(w_avg[\"features.2.conv.1.1.weight\"][0]))\n",
        "\n",
        "#MAIN MODEL -> CLIENTS\n",
        "def send_main_model_to_nodes_and_update_clients(main_model, clients):\n",
        "    with torch.no_grad():\n",
        "      w=main_model.state_dict()\n",
        "      for i in range(len(clients)):\n",
        "        clients[i].net.load_state_dict(copy.deepcopy(w))\n",
        "    return clients\n",
        "\n",
        "\n",
        "\n",
        "#TRAIN ALL CLIENTS\n",
        "def start_train_nodes(clients):\n",
        "    for i in range(len(clients)): \n",
        "        clients[i].net = clients[i].net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "        previousW=copy.deepcopy(clients[i].net.state_dict())  #save the weights     θ ← θt\n",
        "\n",
        "        cudnn.benchmark # Calling this optimizes runtime\n",
        "        for epoch in range(clients[i].local_epoch):    \n",
        "            for images, labels in clients[i].train_loader:\n",
        "              # Bring data over the device of choice\n",
        "              images = images.to(DEVICE)\n",
        "              labels = labels.to(DEVICE)\n",
        "\n",
        "              clients[i].net.train() # Sets module in training mode\n",
        "\n",
        "              clients[i].optimizer.zero_grad() # Zero-ing the gradients\n",
        "              # Forward pass to the network\n",
        "              outputs = clients[i].net(images)\n",
        "              # Compute loss based on output and ground truth\n",
        "              loss = clients[i].criterion(outputs, labels)\n",
        "\n",
        "              # Compute gradients for each layer and update weights\n",
        "              loss.backward()  # backward pass: computes gradients\n",
        "              clients[i].optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "        #calculate update   Δθ ← θt - θ\n",
        "        updates=clients[i].net.state_dict()\n",
        "        for key in updates.keys():\n",
        "          updates[key] = updates[key]-previousW[key]\n",
        "        clients[i].updates=copy.deepcopy(updates)\n",
        "    return clients\n",
        "\n",
        "\n",
        "#WEIGHTED ACCURACY\n",
        "def weighted_accuracy(clients):\n",
        "  sum=0\n",
        "  num_samples=0\n",
        "  for i in range(len(clients)):\n",
        "    test_loss, test_accuracy = evaluate(clients[i].net,clients[i].criterion, clients[i].test_loader)\n",
        "    w=len(clients[i].train_loader.dataset)\n",
        "    num_samples=num_samples+w\n",
        "\n",
        "    sum=sum+test_accuracy*w\n",
        "  return sum/num_samples\n",
        "\n",
        "\n",
        "#SELECT CLIENTS\n",
        "def selectClients(clients):\n",
        "  for i in range(random.randint(2,7)):\n",
        "    random.shuffle(clients)\n",
        "\n",
        "  round_clients_list=[]\n",
        "  for i in range(NUM_SELECTED_CLIENTS):\n",
        "    round_clients_list.append(clients[i])\n",
        "  return round_clients_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apwItqhtyzJ0"
      },
      "source": [
        "TRAIN THE SYSTEM AND TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "oP84Zm_Wsp-L",
        "outputId": "1c226bea-f161-4bac-f48f-94ebeaddc2ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: LeNet5\n",
            "Dataset distribution: 3  1.00 (1,7)\n",
            "Number of clients: 100\n",
            "Number of selected clients: 3\n",
            "Number of rounds: 20\n",
            "-----------------------------------------\n",
            "After round 1  main model accuracy: 0.1    weighted accuracy: 0.095   time: 2.09 sec.\n",
            "After round 2  main model accuracy: 0.1    weighted accuracy: 0.095   time: 1.96 sec.\n",
            "After round 3  main model accuracy: 0.122    weighted accuracy: 0.13   time: 2.05 sec.\n",
            "After round 4  main model accuracy: 0.1    weighted accuracy: 0.1   time: 2.09 sec.\n",
            "After round 5  main model accuracy: 0.138    weighted accuracy: 0.146   time: 1.95 sec.\n",
            "After round 6  main model accuracy: 0.117    weighted accuracy: 0.119   time: 2.08 sec.\n",
            "After round 7  main model accuracy: 0.103    weighted accuracy: 0.098   time: 1.97 sec.\n",
            "After round 8  main model accuracy: 0.133    weighted accuracy: 0.14   time: 2.01 sec.\n",
            "After round 9  main model accuracy: 0.189    weighted accuracy: 0.192   time: 2.16 sec.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-f800331fad47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;31m#TEST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_model_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmain_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m   \u001b[0mmain_model_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_model_accuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0mw_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclients_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-66448ec89c27>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(net, criterion, dataloader, print_tqdm)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m#iterable = tqdm(dataloader) if print_tqdm else dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m       \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mmode_to_nptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'I'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I;16'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     img = torch.from_numpy(\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_to_nptype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "print(\"Model: \"+str(MODEL))\n",
        "print(\"Dataset distribution: \"+str(distribution)+\"  \"+str(alpha)+\" (\"+str(NUM_CLASS_RANGE[0])+','+str(NUM_CLASS_RANGE[1])+')')\n",
        "print(\"Number of clients: \"+str(NUM_CLIENTS))\n",
        "print(\"Number of selected clients: \"+str(NUM_SELECTED_CLIENTS))\n",
        "print(\"Number of rounds: \"+str(ROUNDS))\n",
        "print(\"-----------------------------------------\")\n",
        "\n",
        "#INSTANCE MAIN MODEL\n",
        "main_model=get_net()\n",
        "main_optimizer = torch.optim.SGD(main_model.parameters(), lr=SERVER_LR, momentum=MOMENTUM)\n",
        "main_criterion = nn.CrossEntropyLoss()\n",
        "main_model = main_model.to(DEVICE)\n",
        "\n",
        "#MAIN MODEL -> CLIENTS\n",
        "clients_list=send_main_model_to_nodes_and_update_clients(main_model, clients_list)\n",
        "\n",
        "for i in range(ROUNDS):\n",
        "  start_time = time.time()\n",
        "  if(FL_MODE==0):\n",
        "    break\n",
        "\n",
        "  #SELECT CLEINTS\n",
        "  round_clients_list=selectClients(clients_list)\n",
        "\n",
        "  #TRAIN CLIENTS\n",
        "  round_clients_list=start_train_nodes(round_clients_list)\n",
        "\n",
        "  #CLIENTS -> MAIN MODEL & AVERAGE\n",
        "  main_model= set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,round_clients_list)\n",
        "\n",
        "  #DEBUG\n",
        "  debug=0\n",
        "  if(debug):\n",
        "    print(\"\")\n",
        "    printWeights(round_clients_list,main_model)\n",
        "\n",
        "  #MAIN MODEL -> CLIENTS\n",
        "  clients_list=send_main_model_to_nodes_and_update_clients(main_model, clients_list)\n",
        "\n",
        "  #TEST\n",
        "  test_loss, main_model_accuracy = evaluate(main_model,main_criterion, test_loader)\n",
        "  main_model_accuracy=round(main_model_accuracy,3)\n",
        "  w_accuracy=round(weighted_accuracy(clients_list),3)\n",
        "  seconds=str(round(float(time.time() - start_time),2))\n",
        "  print(\"After round \"+str(i+1)+\"  main model accuracy: \"+str(main_model_accuracy)+\"    weighted accuracy: \"+str(w_accuracy)+\"   time: \"+seconds+\" sec.\")\n",
        "\n",
        "print(\"-----------------------------------------\")\n",
        "print(\"Final Accuracy of Main Model Federated Learning: \"+str(main_model_accuracy))\n",
        "print(\"Final Weighted Accuracy of Federated Learning: \"+str(w_accuracy))\n",
        "print(\"Final Accuracy of standard approach: \"+str(centralized_accuracy))\n",
        "print(\"//////////////////////////////////////////////////////////////////\")\n",
        "print(\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "FedAvg Baseline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}